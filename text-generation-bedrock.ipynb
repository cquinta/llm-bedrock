{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with zero shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking a Model \n",
    "\n",
    "The code above will invoke a model for text generation with zero prompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the prompt\n",
    "prompt_data = \"\"\"\n",
    "Command: Write an email from Bob, Customer Service Manager, to the customer \"John Doe\" \n",
    "who provided negative feedback on the service provided by our customer support \n",
    "engineer\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building de body for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data, \n",
    "    \"textGenerationConfig\":{\n",
    "        \"maxTokenCount\":4096,\n",
    "        \"stopSequences\":[],\n",
    "        \"temperature\":0,\n",
    "        \"topP\":0.9\n",
    "        }\n",
    "    }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Model\n",
    "First explore how model generates output based on the prompt created earlier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = 'amazon.titan-tg1-large' # change this to use a different version from the model provider\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "outputText = \"\\n\"\n",
    "try:\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    outputText = response_body.get('results')[0].get('outputText')\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Apology for the inconvenience caused\n",
      "\n",
      "Dear John Doe,\n",
      "\n",
      "I hope this email finds you well. I am writing to express my sincere apologies for the inconvenience caused by the service provided by our customer support engineer.\n",
      "\n",
      "We understand that your experience with us was not up to the mark, and we take your feedback seriously. We have thoroughly investigated the matter and taken steps to prevent such incidents from happening in the future.\n",
      "\n",
      "I would like to assure you that we are committed to providing excellent customer service and resolving any issues you may have. Our team will do everything they can to assist you and ensure that your experience with us is positive.\n",
      "\n",
      "Please accept our apologies once again for the inconvenience caused. If you have any further concerns or questions, please do not hesitate to contact us.\n",
      "\n",
      "Sincerely,\n",
      "Bob\n",
      "Customer Service Manager\n"
     ]
    }
   ],
   "source": [
    "# The relevant portion of the response begins after the first newline character\n",
    "# Below we print the response beginning after the first occurence of '\\n'.\n",
    "\n",
    "email = outputText[outputText.index('\\n')+1:]\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\u001b[31m**Chunk 1**\u001b[0m\n",
      "\n",
      "Subject: Apology for the inconvenience caused\n",
      "\n",
      "Dear John Doe,\n",
      "\n",
      "I hope this email finds you well. I am writing to express my sincere apologies for the inconvenience caused by the service provided by our \n",
      "\n",
      "\t\t\u001b[31m**Chunk 2**\u001b[0m\n",
      "customer support engineer.\n",
      "\n",
      "We understand that your experience with us was not up to the mark, and we take your feedback seriously. We have thoroughly investigated the matter and taken steps to prevent such incidents from happening in the future.\n",
      "\n",
      "I would like to assure you that we are committed to providing excellent customer service and resolving any issues\n",
      "\n",
      "\t\t\u001b[31m**Chunk 3**\u001b[0m\n",
      " you may have. Our team will do everything they can to assist you and ensure that your experience with us is positive.\n",
      "\n",
      "Please accept our apologies once again for the inconvenience caused. If you have any further concerns or questions, please do not hesitate to contact us.\n",
      "\n",
      "Sincerely,\n",
      "Bob\n",
      "Customer Service Manager\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "try:\n",
    "    \n",
    "    response = bedrock_client.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    stream = response.get('body')\n",
    "    \n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                text = chunk_obj['outputText']\n",
    "                output.append(text)\n",
    "                print(f'\\t\\t\\x1b[31m**Chunk {i}**\\x1b[0m\\n{text}\\n')\n",
    "                i+=1\n",
    "            \n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\u001b[31m**COMPLETE OUTPUT**\u001b[0m\n",
      "\n",
      "\n",
      "Subject: Apology for the inconvenience caused\n",
      "\n",
      "Dear John Doe,\n",
      "\n",
      "I hope this email finds you well. I am writing to express my sincere apologies for the inconvenience caused by the service provided by our customer support engineer.\n",
      "\n",
      "We understand that your experience with us was not up to the mark, and we take your feedback seriously. We have thoroughly investigated the matter and taken steps to prevent such incidents from happening in the future.\n",
      "\n",
      "I would like to assure you that we are committed to providing excellent customer service and resolving any issues you may have. Our team will do everything they can to assist you and ensure that your experience with us is positive.\n",
      "\n",
      "Please accept our apologies once again for the inconvenience caused. If you have any further concerns or questions, please do not hesitate to contact us.\n",
      "\n",
      "Sincerely,\n",
      "Bob\n",
      "Customer Service Manager\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t\\x1b[31m**COMPLETE OUTPUT**\\x1b[0m\\n')\n",
    "complete_output = ''.join(output)\n",
    "print(complete_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With a prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Bedrock LLM Model using Langchain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello!', additional_kwargs={'usage': {'prompt_tokens': 11, 'completion_tokens': 4, 'total_tokens': 15}}, response_metadata={'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0', 'usage': {'prompt_tokens': 11, 'completion_tokens': 4, 'total_tokens': 15}}, id='run-dfb7238c-4883-4e37-b4e2-f9cd76d914ba-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%pip install langchain-aws\n",
    "from langchain_aws import ChatBedrock as Bedrock\n",
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "inference_modifier = { \n",
    "                      \"temperature\":0.5,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "chat = Bedrock(model_id = modelId, model_kwargs = inference_modifier)\n",
    "\n",
    "chat.invoke(\"Helo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a LangChain Custom Prompt\n",
    "Now we will create a langchain custom prompt template with different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"customerServiceManager\", \"customerName\", \"feedbackFromCustomer\"], \n",
    "    template=\"\"\"\n",
    "\n",
    "Human: Create an apology email from the Service Manager {customerServiceManager} to {customerName} in response to the following feedback that was received from the customer: \n",
    "<customer_feedback>\n",
    "{feedbackFromCustomer}\n",
    "</customer_feedback>\n",
    "\n",
    "Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(customerServiceManager=\"Bob\", \n",
    "                                 customerName=\"John Doe\", \n",
    "                                 feedbackFromCustomer=\"\"\"Hello Bob,\n",
    "     I am very disappointed with the recent experience I had when I called your customer support.\n",
    "     I was expecting an immediate call back but it took three days for us to get a call back.\n",
    "     The first suggestion to fix the problem was incorrect. Ultimately the problem was fixed after three days.\n",
    "     We are very unhappy with the response provided and may consider taking our business elsewhere.\n",
    "     \"\"\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prompt has 127 tokens\n"
     ]
    }
   ],
   "source": [
    "#%pip install anthropic\n",
    "num_tokens = chat.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Again\n",
    "Now we will invoke the model with the customized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear John Doe,\n",
      "\n",
      "Please accept my sincere apologies for the unsatisfactory experience you had with our customer support. We strive to provide excellent service, and it is clear that we fell short of meeting your expectations.\n",
      "\n",
      "I want to assure you that the delay in responding promptly and the incorrect initial suggestion are not up to our standards. We will thoroughly investigate the circumstances that led to this situation and take appropriate measures to prevent such occurrences in the future.\n",
      "\n",
      "Your satisfaction is our top priority, and we value your business greatly. I understand your frustration and the potential consideration of taking your business elsewhere. However, I hope that you will give us another opportunity to regain your trust and demonstrate our commitment to delivering outstanding service.\n",
      "\n",
      "Please feel free to reach out to me directly if you have any further concerns or if there is anything else we can do to make this right. We are determined to learn from this experience and improve our processes to ensure that our valued customers receive the prompt and effective support they deserve.\n",
      "\n",
      "Thank you for bringing this matter to my attention. I appreciate your feedback, as it helps us identify areas where we can enhance our service delivery.\n",
      "\n",
      "Sincerely,\n",
      "Bob\n",
      "Service Manager\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
